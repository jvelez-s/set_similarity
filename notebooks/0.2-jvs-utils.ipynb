{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.2 Utils\n",
    "\n",
    "In general, a package is made that contains the useful functions and those that are specific to the problem to be solved, however, in the case of red-hot development it can be useful to have all the functions available in each notebook that is created, but without extra effort required to create a package. If, at some point, the goal of the project is to make a pipeline or a program that helps to perform a certain type of analysis, considering creating a formal package would be the most appropriate. Therefore, useful functions will be created here that will eventually be used by all the notebooks that are created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Â Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import functools\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "\n",
    "from pyprojroot import here\n",
    "from typing import (\n",
    "    Any,\n",
    "    Callable,\n",
    "    Dict,\n",
    "    Iterable,\n",
    "    Set,\n",
    "    Union,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function declarations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paths\n",
    "\n",
    "For this project, the `here` function is used to establish the root of our project and maintain amore orderly working environment.  \n",
    "> **Note:** `here` works like `file.path` on R or `pathlib.Path` on python, but where the path root is implicitly set to **\"the path to the top-level of my current project\".** It looks at working directory, checks a criterion and, if not satisfied, moves up to parent directory and checks agains. Lather, rinse, repeat. See [here](https://github.com/jennybc/here_here) for **R** or [here](https://pypi.org/project/pyprojroot/) for **Python**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_directories(prefix: str=\"_\") -> dict:\n",
    "    \"\"\"Automatically identify directories in current project.\n",
    "    \n",
    "    The 'here()' function uses a reasonable heuristics to find your project's files,\n",
    "    based on the current working directory at the time when the package is loaded.\n",
    "    \n",
    "    Starting from the identification of a project, it proceeds to identify all the\n",
    "    non-hidden subdirectories.\n",
    "    \n",
    "    Args:\n",
    "        prefix: A string to use as prefix for the keys on dictionary.\n",
    "    \n",
    "    Returns:\n",
    "        A dict mapping keys to the corresponding paths of the folders found in the project.\n",
    "        \n",
    "        A momentary disadvantage is that if there are subdirectories with the same names\n",
    "        it will only identify one of them. The function can be updated to deal with it.\n",
    "    \"\"\"\n",
    "    \n",
    "    directories = {\n",
    "        '_' + path.stem: path\n",
    "        for path in here().rglob('**')\n",
    "        if path.is_dir() and not path.anchor + '.' in str(path)\n",
    "    }\n",
    "    \n",
    "    return directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dir_function(dirname: Union[str, Iterable[str]]) -> Callable:\n",
    "    \"\"\"Generates a function that converts a string or iterable of strings into a path relative to the project directory.\n",
    "    \n",
    "    Args:\n",
    "        dirname: Name of the subdirectories to extend the path of the main project.\n",
    "            If an iterable of strings is passed as an argument, then it is collapsed\n",
    "            to a single string with anchors dependent on the operating system.\n",
    "    \n",
    "    Returns:\n",
    "        Function that returns the path relative to a directory that can receive n number of arguments for expansion. \n",
    "    \"\"\"\n",
    "    \n",
    "    def dir_path(*args) -> pathlib.PosixPath:\n",
    "        \n",
    "        if type(dirname) == str:\n",
    "            return here().joinpath(dirname, *args)\n",
    "        else:\n",
    "            return here().joinpath(*dirname, *args)\n",
    "        \n",
    "    return dir_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots\n",
    "\n",
    "Before you start graphing the data to explore how it relates, it's good to think about style consistencies for a unified job. Therefore, making a section to configure the default values for the charts is appreciable. See [here](https://matplotlib.org/3.3.1/tutorials/introductory/customizing.html) for more information about the parameters that can be modified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_plot_defaults(defaults: dict = None, style :str= None) -> None:\n",
    "    \"\"\"Updates the default plotting parameters in matplotlib.\n",
    "    \n",
    "    Args:\n",
    "        defaults: Dictionary where the keys map to a matplotlib backend parameter (see `plt.rcParams` for possibilities).\n",
    "        style: The style package adds support for easy-to-switch plotting \"styles\" with the same parameters as a matplotlib\n",
    "        `rc file` (which is read at startup to configure Matplotlib).\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "    if style is None:\n",
    "        plt.style.use('seaborn-whitegrid')\n",
    "    else:\n",
    "        plt.style.use(style)\n",
    "        \n",
    "    if defaults is None:\n",
    "        defaults = {\n",
    "            'legend.loc': 'best',\n",
    "            'font.family': ['sans-serif'],\n",
    "            'figure.figsize': [10.0, 8.0],\n",
    "            'legend.shadow': True,\n",
    "            'axes.labelsize': 14,\n",
    "        }\n",
    "\n",
    "    for key, value in defaults.items():\n",
    "        plt.rcParams[key] = value\n",
    "        \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_split_to_dict(text: str, sep: str= ',', to_set: bool=True) -> collections.defaultdict(Union[set, list]):\n",
    "    \"\"\"Separates a text in a dictionary of sets or lists.\n",
    "    \n",
    "    Args:\n",
    "        text: Text string that will be separated in a dictionary.\n",
    "        sep: String to split text.\n",
    "        to_set: Dictionary values should be stored in a list or a set?\n",
    "    \n",
    "    Returns:\n",
    "        A dictionary with keys specified by position in the text and values assigned as a list or a set.\n",
    "    \"\"\"\n",
    "    \n",
    "    kind = set if to_set else list\n",
    "    \n",
    "    out_dict = collections.defaultdict(kind)\n",
    "        \n",
    "    for i, text_split in enumerate(text.split(sep), 1):\n",
    "        \n",
    "        out_dict[i].add(text_split) if to_set else out_dict[i].append(text_split)\n",
    "        \n",
    "    return out_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def product_entries(A: Iterable, B: Iterable=None, labels: Iterable[Union[str, str]]=None) -> pd.DataFrame:\n",
    "    \"\"\"Create a dataframe with the cartesian product of the supplied values.\n",
    "    \n",
    "    Args:\n",
    "        A: Values to use.\n",
    "        B: Optional; If specified, the values of B are used in conjunction with those of A.\n",
    "        labels: Optional; Names for column A and B. If not specified then the output columns\n",
    "            will be named A and B.\n",
    "        \n",
    "    Returns:\n",
    "        Dataframe with the cartesian product of the supplied values.\n",
    "    \"\"\"\n",
    "    \n",
    "    entries = itertools.product(A, A) if B is None else itertools.product(A, B)\n",
    "    \n",
    "    if labels is None:\n",
    "        labels = [\"A\", \"B\"]\n",
    "    \n",
    "    df = (\n",
    "        pd.DataFrame(entries).\n",
    "        rename(columns={0: labels[0], 1: labels[1]}).\n",
    "        sort_values(by=labels, ignore_index=True)\n",
    "    )\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numeric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jaccard calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard(\n",
    "    A: set,\n",
    "    B: set,\n",
    "    error_return: float = 0.0,\n",
    "    simmetric: bool = True,\n",
    "    modified=False\n",
    ") -> float:\n",
    "    \"\"\"Returns the jaccard index of sets A and B.\n",
    "    \n",
    "    Args:\n",
    "        A: Set A.\n",
    "        B: Set B.\n",
    "        error_return: Optional; If division by zero occurs, use the specified value as a replacement.\n",
    "        simmetric: Optional; If it is true, the division is on the length of the union of both sets,\n",
    "            otherwise it is done on the length of the set A.\n",
    "        modified: Optional; If true, divide over the smallest set and ignore the `simmetric` parameter.\n",
    "        \n",
    "    Returns:\n",
    "        Jaccar index of sets A and B.\n",
    "    \"\"\"\n",
    "    \n",
    "    numerator = float(len(A.intersection(B)))\n",
    "    \n",
    "    if modified:\n",
    "        denominator = min([len(A), len(B)])\n",
    "    else:\n",
    "        denominator = len(A.union(B)) if simmetric else len(A)\n",
    "\n",
    "    try:\n",
    "        return numerator / denominator\n",
    "    except ZeroDivisionError:\n",
    "        return error_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nested_jaccard(\n",
    "    A: Dict[Any, set],\n",
    "    B: Dict[Any, set],\n",
    "    not_key_return: Any=-1,\n",
    "    global_label: str=None,\n",
    "    **kwargs\n",
    ") -> Dict[Any, float]:\n",
    "    \"\"\"Returns the jaccard index of sets contained in A and B.\n",
    "    \n",
    "    Args:\n",
    "        A: Set A.\n",
    "        B: Set B.\n",
    "        not_key_return: Default values in case the key is not shared in both dictionaries.\n",
    "        kwargs: Arguments passed to the `jaccard` function.\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary where the keys correspond to the union of keys of dictionaries A and B\n",
    "        and the values are the jaccard index for each entry.\n",
    "    \"\"\"\n",
    "    \n",
    "    score_dict = {}\n",
    "    \n",
    "    if global_label is not None:\n",
    "        \n",
    "        set_A, set_B = [functools.reduce(set.union, current_dict.values()) for current_dict in [A, B]]\n",
    "        score_dict[global_label] = jaccard(set_A, set_B, **kwargs)\n",
    "    \n",
    "    for key in set([*A.keys(), *B.keys()]):\n",
    "        \n",
    "        set_A, set_B = A.get(key, None), B.get(key, None)\n",
    "        \n",
    "        if type(set_A) is not set or type(set_B) is not set:\n",
    "            score_dict[key] = not_key_return\n",
    "        else:\n",
    "            score_dict[key] = jaccard(set_A, set_B, **kwargs)\n",
    "\n",
    "    return score_dict    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jaccard applied with Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_df(\n",
    "    df: pd.DataFrame,\n",
    "    set_column: str,\n",
    "    matrix: bool=False,\n",
    "    B=None,\n",
    "    labels: Iterable[Union[str, str]]=None,\n",
    "    **kwargs\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Returns the jacdard index of an indexed dataframe with a column with the sets to use.\n",
    "    \n",
    "    Args:\n",
    "        df: Indexed dataframe with a column of sets to use.\n",
    "        set_column: Name of the column with the sets to use for the jaccard index.\n",
    "        matrix: Optional; Should the result be presented in a long or tidy format?\n",
    "        B: Use the Cartesian product of the dataframe index in conjunction with the supplied values.\n",
    "        labels: Optional; Names for column A and B. If not specified then the output columns\n",
    "            will be named A and B.\n",
    "        kwargs: Extra arguments to pass to the `jaccard` function.\n",
    "        \n",
    "    Returns:\n",
    "        Dataframe with the cartesian product of the supplied values and their respective jaccard index values.\n",
    "    \"\"\"\n",
    "    \n",
    "    if labels is None:\n",
    "        labels = [\"A\", \"B\"]\n",
    "    \n",
    "    df_score = (\n",
    "        product_entries(A=df.index, B=B, labels=labels).\n",
    "        assign(\n",
    "            A_set=lambda x: df.loc[x[labels[0]], set_column].values,\n",
    "            B_set=lambda x: df.loc[x[labels[1]], set_column].values,\n",
    "            score=lambda x: x.apply(lambda row: jaccard(row[\"A_set\"], row[\"B_set\"], **kwargs), axis=1)\n",
    "        ).\n",
    "        drop(columns=[\"A_set\", \"B_set\"])\n",
    "    )\n",
    "    \n",
    "    return df_score if not matrix else df_score.pivot_table(index=[labels[0]], columns=[labels[1]])[\"score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nested_jaccard_df(\n",
    "    df: pd.DataFrame,\n",
    "    set_column: str,\n",
    "    matrix: bool=False,\n",
    "    B=None,\n",
    "    labels: Iterable[Union[str, str]]=None,\n",
    "    **kwargs\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Returns the jaccard index of an indexed dataframe with a column with the sets to use.\n",
    "    \n",
    "    Args:\n",
    "        df: Indexed dataframe with a column of sets to use.\n",
    "        set_column: Name of the column with the sets to use for the jaccard index.\n",
    "        matrix: Optional; Should the result be presented in a long or tidy format?\n",
    "        B: Use the Cartesian product of the dataframe index in conjunction with the supplied values.\n",
    "        labels: Optional; Names for column A and B. If not specified then the output columns\n",
    "            will be named A and B.\n",
    "        kwargs: Extra arguments to pass to the `jaccard` function.\n",
    "        \n",
    "    Returns:\n",
    "        Dataframe with the cartesian product of the supplied values and their respective jaccard index values.\n",
    "    \"\"\"\n",
    "    \n",
    "    if labels is None:\n",
    "        labels = [\"A\", \"B\"]\n",
    "    \n",
    "    df_score = (\n",
    "        product_entries(A=df.index, B=B, labels=labels).\n",
    "        assign(\n",
    "            A_set=lambda x: df.loc[x[labels[0]], set_column].values,\n",
    "            B_set=lambda x: df.loc[x[labels[1]], set_column].values,\n",
    "            tmp_scores=lambda x: x.apply(lambda row: nested_jaccard(row[\"A_set\"], row[\"B_set\"], **kwargs), axis=1)\n",
    "        ).\n",
    "        pipe(lambda df: pd.concat([df, pd.json_normalize(df.tmp_scores)], axis=1)).\n",
    "        drop(columns=[\"A_set\", \"B_set\", \"tmp_scores\"])\n",
    "        \n",
    "    )\n",
    "    \n",
    "    if \"not_key_return\" in kwargs:\n",
    "        df_score.fillna(value=kwargs[\"not_key_return\"], inplace=True)\n",
    "    \n",
    "    return df_score if not matrix else df_score.pivot_table(index=[labels[0]], columns=[labels[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_nodes(\n",
    "    nodes: Union[list, set],\n",
    "    G: nx.MultiDiGraph,\n",
    "    inmediate: bool=False,\n",
    "    use_edges: str=\"in\"\n",
    ") -> set:\n",
    "    \"\"\"Keep non-redundantnodes in one direction (i.e in, out) presented in the acyclic directed graph.\n",
    "    \n",
    "    Args:\n",
    "        nodes: Nodes to reduce.\n",
    "        G: Graph to search.\n",
    "        inmediate: Optional; If true, it uses only the nodes with direct connection to perform the filtering,\n",
    "            otherwise it uses all the reachable nodes.\n",
    "        use_edges: Optional; Filtering direction. If the value is `in`, it uses all the ancestors of the nodes\n",
    "            and preserves the non-reducing child nodes, otherwise it uses all the descendant nodes, preserving\n",
    "            all the non-redudant parent nodes.\n",
    "        \n",
    "    Returns:\n",
    "        Set of non-redudant nodes.\n",
    "    \"\"\"\n",
    "\n",
    "    if use_edges not in (\"in\", \"out\"):\n",
    "        raise ValueError(\"use_edges must be in ('in', 'out').\")    \n",
    "\n",
    "    nodes = set(nodes)\n",
    "\n",
    "    if inmediate:\n",
    "        \n",
    "        edges, use = (G.in_edges, 0) if use_edges == \"in\" else (G.out_edges, 1)\n",
    "        related_nodes = set(edge[use] for edge in edges(nodes))\n",
    "\n",
    "    else:\n",
    "\n",
    "        connection_function = nx.ancestors if use_edges == \"in\" else nx.descendants\n",
    "        related_nodes = functools.reduce(set.union, [connection_function(G=G, source=node) for node in nodes])\n",
    "    \n",
    "    return nodes.difference(nodes.intersection(related_nodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_iter_dicts(iter_dicts: Iterable[Dict[Any, Set[Any]]], to_set: bool=True) -> Dict[Any, Set[Any]]:\n",
    "    \"\"\"Merge dictionaries and keep values of common keys in a set.\n",
    "    \"\"\"\n",
    "    \n",
    "    def merge_dictionaries(dict_1, dict_2):\n",
    "        \"\"\"\"\"\"\n",
    "        nonlocal to_set\n",
    "        \n",
    "        kind = set if to_set else list\n",
    "        merged_dictonary = collections.defaultdict(kind)\n",
    "        for key, value in itertools.chain(dict_1.items(), dict_2.items()):\n",
    "            value = value if isinstance(value, set) else [value]\n",
    "            merged_dictonary[key].update(value) if to_set else merged_dictonary.extend(value)\n",
    "        \n",
    "        return merged_dictonary\n",
    "    \n",
    "    return functools.reduce(merge_dictionaries, iter_dicts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
